from sagemaker.pytorch import PyTorch
import sagemaker

role = 'arn:aws:iam::552401576656:role/SageMakerExecutionRole'
session = sagemaker.Session()

# Cost optimization options
USE_SPOT_INSTANCES = False  # Set to False for guaranteed on-demand instances
INSTANCE_TYPE = 'ml.g6.8xlarge'  # GPU instance for faster training

# Enhanced training configuration with early stopping and adaptive loss
hyperparameters = {
    'train_json': '/opt/ml/input/data/train_augmented/augmented_train.json',
    'val_json': '/opt/ml/input/data/train_augmented/augmented_val.json', 
    'image_root': '/opt/ml/input/data/train_augmented',
    'model_size': 'base',  # tiny, small, base, large
    'epochs': 150,  # Higher since early stopping will prevent overfitting
    'batch_size': 32,
    'learning_rate': 0.0001,
    'weight_decay': 0.05,
    'warmup_epochs': 10,
    'output_dir': '/opt/ml/model',
    'validate_every': 1,  # Validate every epoch
    'detailed_metrics': 'true',  # Enable F1, precision, recall
    'save_best_only': 'true',    # Only save best model to save space
    'early_stopping_patience': 20,  # Stop if no improvement for 20 epochs
    'adaptive_loss_weighting': 'true',  # Auto-balance company vs model loss
}

print("ğŸ¦· Enhanced Company + Model Classification Training")
print("=" * 60)
print("ğŸ¯ Enhanced production features:")
print("   â€¢ Company identification") 
print("   â€¢ Model identification")
print("   â€¢ Early stopping (prevents overfitting)")
print("   â€¢ Adaptive loss weighting (auto-balances tasks)")
print("   â€¢ Learning rate warmup & cosine annealing")
print("   â€¢ Label smoothing & gradient clipping") 
print("   â€¢ Detailed validation metrics (F1, precision, recall)")
print("   â€¢ Enhanced data augmentation")
print("   â€¢ Best model auto-saving")
print("=" * 60)

print(f"\nğŸ’° Cost Configuration:")
if USE_SPOT_INSTANCES:
    print(f"   ğŸ¯ Spot Instances: ENABLED (70-90% cost savings)")
    print(f"   âš ï¸  Risk: Training may be interrupted (rare)")
    print(f"   ğŸ”„ Auto-restart: Enabled with checkpointing")
    print(f"   ğŸ’¡ Best for: Development, experimentation")
else:
    print(f"   ğŸ’° On-Demand Instances: Guaranteed but expensive")
    print(f"   âœ… No interruption risk")
    print(f"   ğŸ’¡ Best for: Production, critical deadlines")

print(f"   ğŸ–¥ï¸  Instance: {INSTANCE_TYPE}")

# Estimate costs
if INSTANCE_TYPE == 'ml.g6.8xlarge':
    on_demand_cost = 4.352  # $/hour
    spot_cost = on_demand_cost * 0.3  # ~70% savings typical
    training_hours = 4  # estimated with early stopping
    
    print(f"\nğŸ’µ Cost Estimate for ~{training_hours} hours (with early stopping):")
    print(f"   On-demand: ${on_demand_cost * training_hours:.2f}")
    if USE_SPOT_INSTANCES:
        print(f"   Spot price: ~${spot_cost * training_hours:.2f} (70% savings)")
        print(f"   ğŸ’° You save: ~${(on_demand_cost - spot_cost) * training_hours:.2f}")

print("\nğŸš€ Enhanced Training Configuration")
print("=" * 40)
for key, value in hyperparameters.items():
    print(f"{key}: {value}")
print("=" * 40)

# Create estimator with spot instance configuration
estimator_config = {
    'entry_point': 'train_vit_multitask.py',
    'source_dir': '.',
    'role': role,
    'framework_version': '2.1.0',
    'py_version': 'py310',
    'instance_type': INSTANCE_TYPE,
    'instance_count': 1,
    'hyperparameters': hyperparameters,
    'dependencies': ['requirements.txt'],
    'base_job_name': 'enhanced-company-model-spot' if USE_SPOT_INSTANCES else 'enhanced-company-model',
    'output_path': f's3://{session.default_bucket()}/enhanced-company-model-output',
    'volume_size': 50,
}

# Add spot instance configuration
if USE_SPOT_INSTANCES:
    estimator_config.update({
        'use_spot_instances': True,
        'max_run': 16*60*60,  # 16 hours max training time
        'max_wait': 24*60*60,  # 24 hours max wait (including interruptions)
        'checkpoint_s3_uri': f's3://{session.default_bucket()}/enhanced-company-model-checkpoints/',
    })
    print(f"ğŸ”„ Checkpointing enabled: {estimator_config['checkpoint_s3_uri']}")
else:
    estimator_config.update({
        'max_run': 12*60*60,  # Shorter due to early stopping
        'keep_alive_period_in_seconds': 30*60,
    })

estimator = PyTorch(**estimator_config)

# Input data configuration
input_data = {
    'train_augmented': 's3://codentist-general/datasets/aug_dataset_2/'
}

print(f"\nğŸ“Š Input Data Mapping:")
for channel, s3_path in input_data.items():
    print(f"  {channel} -> {s3_path}")

print(f"\nğŸ¯ Expected S3 structure:")
print(f"  s3://codentist-general/datasets/aug_dataset/")
print(f"  â”œâ”€â”€ train/")
print(f"  â”‚   â”œâ”€â”€ Company_A/")
print(f"  â”‚   â”‚   â”œâ”€â”€ image_aug_001.jpg")
print(f"  â”‚   â”‚   â””â”€â”€ labels/")
print(f"  â”‚   â””â”€â”€ Company_B/")
print(f"  â”œâ”€â”€ val/")
print(f"  â”‚   â””â”€â”€ Company_C/")
print(f"  â”œâ”€â”€ augmented_train.json")
print(f"  â”œâ”€â”€ augmented_val.json")
print(f"  â””â”€â”€ augmentation_summary.json")

print(f"\nğŸ¯ Enhanced Training Features:")
print(f"  ğŸ›‘ Early Stopping: {hyperparameters['early_stopping_patience']} epoch patience")
print(f"  ğŸ”§ Adaptive Loss Weighting: Auto-balances company vs model tasks")
print(f"  ğŸ”¬ Learning Rate Warmup: {hyperparameters['warmup_epochs']} epochs")
print(f"  ğŸ“Š Detailed Metrics: Classification reports, F1 scores")
print(f"  ğŸ¯ Target Performance: >95% company, >90% model accuracy")
print(f"  ğŸ’¾ Smart Saving: Only best model saved")
print(f"  ğŸ”„ Validation: Every epoch with comprehensive metrics")

print(f"\nğŸš€ Training Timeline Estimate:")
print(f"  ğŸ• Enhanced ViT-base on ml.g6.8xlarge:")
print(f"     â€¢ Early stopping typically triggers: ~60-80 epochs")
print(f"     â€¢ Estimated training time: ~3-4 hours")
print(f"     â€¢ Warmup phase: First 10 epochs")
print(f"     â€¢ Automatic loss balancing throughout training")

print(f"\nğŸš€ Starting enhanced training job...")
if USE_SPOT_INSTANCES:
    print(f"ğŸ’° Using SPOT INSTANCES for maximum cost savings!")
    print(f"âš ï¸  Note: Training may be interrupted but will auto-resume")
else:
    print(f"ğŸ’° Using ON-DEMAND INSTANCES for guaranteed availability")

# Start the training job
estimator.fit(input_data)

print(f"\nâœ… Enhanced training job submitted!")
if USE_SPOT_INSTANCES:
    print(f"ğŸ’° Spot instance job running - check SageMaker console for status")
    print(f"ğŸ”„ If interrupted, job will automatically restart from checkpoint")
else:
    print(f"ğŸ”’ On-demand job guaranteed to run without interruption")

print(f"ğŸ“Š Monitor progress in SageMaker console")
print(f"ğŸ“ Model artifacts will be saved to: {estimator.output_path}")
if USE_SPOT_INSTANCES:
    print(f"ğŸ”„ Checkpoints saved to: {estimator_config['checkpoint_s3_uri']}")

print(f"\nğŸ“‹ Expected outputs:")
print(f"  ğŸ“¦ best_model.pth - Best performing model with full state")
print(f"  ğŸ“Š vocabularies.json - Company and model mappings") 
print(f"  ğŸ“ˆ training_history.json - Complete training metrics with loss weights")
print(f"  ğŸ“‹ training_info.json - Final summary with early stopping info")

print(f"\nğŸ¯ Enhanced Monitoring Tips:")
if USE_SPOT_INSTANCES:
    print(f"  ğŸ’° Spot price savings: Check 'Billable seconds' vs 'Training time'")
    print(f"  ğŸ”„ Interruptions: Look for 'SpotInterruption' in logs (rare)")
    print(f"  ğŸ“Š Progress preserved: Training resumes from last checkpoint")
else:
    print(f"  ğŸ’° Fixed pricing: Predictable costs, no interruptions")
    
print(f"  ğŸ›‘ Early stopping: Watch for 'Early stopping triggered' message")
print(f"  ğŸ”§ Loss balancing: Monitor company vs model loss weight adjustments")
print(f"  ğŸ“Š Watch for validation accuracy > 0.92 combined")
print(f"  ğŸ” Check CloudWatch logs for detailed metrics")
print(f"  ğŸ’¾ Best model auto-saved when validation improves")

print(f"\nğŸ’¡ Cost Optimization Tips:")
if USE_SPOT_INSTANCES:
    print(f"  âœ… You're already using spot instances (70-90% savings)")
    print(f"  âœ… Early stopping reduces training time automatically")
    print(f"  ğŸ’¡ For even more savings: Try smaller model (--model_size small)")
else:
    print(f"  ğŸ’° Switch to spot instances: Set USE_SPOT_INSTANCES = True")
    print(f"  âœ… Early stopping already reduces costs by ~30-50%")
    print(f"  ğŸ’¡ Use smaller instance: ml.g5.2xlarge for development")

print(f"\nğŸ¯ Success Criteria:")
print(f"  âœ… Company accuracy: >95%")
print(f"  âœ… Model accuracy: >90%") 
print(f"  âœ… Combined accuracy: >92%")
print(f"  âœ… Training converged without overfitting (early stopping)")
print(f"  âœ… F1 scores > 0.90 for both tasks")
print(f"  âœ… Loss weights automatically balanced between tasks")
if USE_SPOT_INSTANCES:
    print(f"  ğŸ’° Cost savings: 70-90% vs on-demand pricing")

print(f"\nğŸ”§ Enhanced Features Benefits:")
print(f"  ğŸ›‘ Early stopping prevents overfitting and saves time/cost")
print(f"  ğŸ”§ Adaptive loss weighting improves balanced performance")
print(f"  ğŸ“Š Better convergence and more stable training")
print(f"  ğŸ’¾ Automatic best model selection")
print(f"  ğŸ“ˆ Detailed training history with loss weight evolution")