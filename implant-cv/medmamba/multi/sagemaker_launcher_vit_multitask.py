from sagemaker.pytorch import PyTorch
import sagemaker

role = 'arn:aws:iam::552401576656:role/SageMakerExecutionRole'
session = sagemaker.Session()
bucket = 'codentist-general'

# Enhanced multi-task training configuration
hyperparameters = {
    'train_json': '/opt/ml/input/data/train_augmented/augmented_train.json',
    'val_json': '/opt/ml/input/data/train_augmented/augmented_val.json', 
    'model_size': 'base',  # tiny, small, base, large
    'epochs': 200,  # Higher since early stopping will prevent overfitting
    'batch_size': 32,
    'learning_rate': 0.0001,
    'weight_decay': 0.05,
    'warmup_epochs': 10,
    'output_dir': '/opt/ml/model',
    'image_root': '/opt/ml/input/data/train_augmented',
    'validate_every': 5,  # Run validation every 5 epochs
    'early_stopping_patience': 20,  # Stop if no improvement for 20 epochs
    'adaptive_loss_weighting': 'true',  # Auto-balance all 4 tasks
    'detailed_metrics': 'true',  # Show per-task metrics
}

print("ğŸ¦· Enhanced Multi-Task Classification Training")
print("=" * 60)
print("ğŸ¯ Enhanced production features:")
print("   â€¢ Company identification") 
print("   â€¢ Model identification")
print("   â€¢ Diameter classification")
print("   â€¢ Length classification")
print("   â€¢ Early stopping (prevents overfitting)")
print("   â€¢ Adaptive loss weighting (auto-balances 4 tasks)")
print("   â€¢ Enhanced data augmentation")
print("   â€¢ Learning rate warmup & cosine annealing")
print("   â€¢ Label smoothing & gradient clipping") 
print("   â€¢ Best model auto-saving")
print("=" * 60)

print("ğŸš€ Enhanced SageMaker Training Configuration")
print("=" * 40)
for key, value in hyperparameters.items():
    print(f"{key}: {value}")
print("=" * 40)

# Estimate costs with early stopping
on_demand_cost = 4.352  # $/hour for ml.g6.8xlarge
training_hours = 4  # estimated with early stopping for 4-task model

print(f"\nğŸ’µ Cost Estimate for ~{training_hours} hours (with early stopping):")
print(f"   Instance: ml.g6.8xlarge - ${on_demand_cost * training_hours:.2f}")
print(f"   ğŸ’° Early stopping typically saves 30-50% vs full training")

estimator = PyTorch(
    entry_point='train_vit_multitask.py',
    source_dir='.',  # Local directory containing your scripts
    role=role,
    framework_version='2.1.0',
    py_version='py310',
    instance_type='ml.g6.8xlarge',  # GPU instance for faster training
    instance_count=1,
    hyperparameters=hyperparameters,
    dependencies=['requirements.txt'],
    base_job_name='enhanced-vit-multitask',
    output_path=f's3://{session.default_bucket()}/enhanced-vit-multitask-output',
    max_run=16*60*60,  # 16 hours max runtime (reduced due to early stopping)
    keep_alive_period_in_seconds=30*60,  # Keep instance alive for 30 min after job
)

# Input data configuration
input_data = {
    'train_augmented': 's3://codentist-general/datasets/aug_dataset/'
}

print(f"\nğŸ“Š Input Data Mapping:")
for channel, s3_path in input_data.items():
    print(f"  {channel} -> {s3_path}")

print(f"\nğŸ¯ Expected S3 structure:")
print(f"  s3://codentist-general/datasets/aug_dataset/")
print(f"  â”œâ”€â”€ train/")
print(f"  â”‚   â”œâ”€â”€ Company_A/")
print(f"  â”‚   â”‚   â”œâ”€â”€ image_aug_001.jpg")
print(f"  â”‚   â”‚   â””â”€â”€ labels/")
print(f"  â”‚   â””â”€â”€ Company_B/")
print(f"  â”œâ”€â”€ val/")
print(f"  â”‚   â””â”€â”€ Company_C/")
print(f"  â”œâ”€â”€ augmented_train.json")
print(f"  â”œâ”€â”€ augmented_val.json")
print(f"  â””â”€â”€ augmentation_summary.json")

print(f"\nğŸ¯ Enhanced Training Features:")
print(f"  ğŸ›‘ Early Stopping: {hyperparameters['early_stopping_patience']} epoch patience")
print(f"  ğŸ”§ Adaptive Loss Weighting: Auto-balances all 4 classification tasks")
print(f"  ğŸ”¬ Learning Rate Warmup: {hyperparameters['warmup_epochs']} epochs")
print(f"  ğŸ“Š Detailed Metrics: Per-task accuracy tracking")
print(f"  ğŸ¯ Multi-Task Performance: Company, Model, Diameter, Length")
print(f"  ğŸ’¾ Smart Saving: Only best model saved")
print(f"  ğŸ”„ Validation: Every {hyperparameters['validate_every']} epochs")

print(f"\nğŸš€ Training Timeline Estimate:")
print(f"  ğŸ• Enhanced 4-task ViT-base on ml.g6.8xlarge:")
print(f"     â€¢ Early stopping typically triggers: ~70-100 epochs")
print(f"     â€¢ Estimated training time: ~4-5 hours")
print(f"     â€¢ Warmup phase: First 10 epochs")
print(f"     â€¢ Automatic loss balancing throughout training")
print(f"     â€¢ All 4 tasks balanced for optimal performance")

print(f"\nğŸš€ Starting enhanced multi-task training job...")

# Start the training job
estimator.fit(input_data)

print(f"\nâœ… Enhanced multi-task training job submitted!")
print(f"ğŸ“Š Monitor progress in SageMaker console")
print(f"ğŸ“ Model artifacts will be saved to: {estimator.output_path}")

print(f"\nğŸ“‹ Expected outputs:")
print(f"  ğŸ“¦ best_model.pth - Best performing model with full state")
print(f"  ğŸ“Š vocabularies.json - All 4 task vocabularies") 
print(f"  ğŸ“ˆ training_history.json - Complete training metrics with loss weights")
print(f"  ğŸ“‹ training_info.json - Final summary with early stopping info")

print(f"\nğŸ¯ Enhanced Monitoring Tips:")
print(f"  ğŸ›‘ Early stopping: Watch for 'Early stopping triggered' message")
print(f"  ğŸ”§ Loss balancing: Monitor 4-task loss weight adjustments")
print(f"  ğŸ“Š Multi-task performance: All tasks should improve together")
print(f"  ğŸ” Check CloudWatch logs for detailed per-task metrics")
print(f"  ğŸ’¾ Best model auto-saved when validation improves")
print(f"  ğŸ“ˆ Training typically converges faster with 4-task learning")

print(f"\nğŸ¯ Success Criteria:")
print(f"  âœ… Company accuracy: >95%")
print(f"  âœ… Model accuracy: >90%") 
print(f"  âœ… Diameter accuracy: >85%")
print(f"  âœ… Length accuracy: >85%")
print(f"  âœ… Training converged without overfitting (early stopping)")
print(f"  âœ… Loss weights automatically balanced between all 4 tasks")
print(f"  ğŸ’° Cost savings: 30-50% vs full training duration")

print(f"\nğŸ”§ Enhanced Features Benefits:")
print(f"  ğŸ›‘ Early stopping prevents overfitting and saves time/cost")
print(f"  ğŸ”§ 4-task adaptive weighting improves balanced performance")
print(f"  ğŸ“Š Better convergence with multi-task learning")
print(f"  ğŸ’¾ Automatic best model selection")
print(f"  ğŸ“ˆ Detailed training history with loss weight evolution")
print(f"  ğŸ¯ Simultaneous optimization of all classification tasks")