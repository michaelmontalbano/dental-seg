name: Build Dataset-Specific nnU-Net Images

on:
  workflow_dispatch:
    inputs:
      datasets:
        description: 'Dataset IDs to build (comma-separated, e.g., 300,302,305 or "all")'
        required: false
        default: 'all'
  push:
    branches:
      - main
    paths:
      - 'nnunet/container_nnunet/**'

env:
  ECR_REPOSITORY: nnunet
  AWS_REGION: us-west-2

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        dataset:
          - {id: 300, name: "BoneLoss"}
          - {id: 301, name: "CoreAnatomicalStructures"}
          - {id: 302, name: "ImplantFeatures"}
          - {id: 303, name: "ClinicalTreatmentPathology"}
          - {id: 304, name: "SpecializedClasses"}
          - {id: 305, name: "AdultTeeth"}
          - {id: 306, name: "PrimaryTeeth"}
          - {id: 307, name: "ToothSurfacesBoundaries"}
    
    steps:
      - name: Check if should build
        id: should_build
        run: |
          DATASETS="${{ github.event.inputs.datasets }}"
          if [ -z "$DATASETS" ] || [ "$DATASETS" = "all" ]; then
            echo "build=true" >> $GITHUB_OUTPUT
          elif echo "$DATASETS" | grep -q "${{ matrix.dataset.id }}"; then
            echo "build=true" >> $GITHUB_OUTPUT
          else
            echo "build=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout repository
        if: steps.should_build.outputs.build == 'true'
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        if: steps.should_build.outputs.build == 'true'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        if: steps.should_build.outputs.build == 'true'
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push Dataset ${{ matrix.dataset.id }}
        if: steps.should_build.outputs.build == 'true'
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          DATASET_ID: ${{ matrix.dataset.id }}
          DATASET_NAME: ${{ matrix.dataset.name }}
        run: |
          cd nnunet/container_nnunet
          
          echo "ðŸ”¨ Building image for Dataset${DATASET_ID}_${DATASET_NAME}..."
          
          # Create a temporary train.py with the correct defaults
          cp train.py train_temp.py
          
          # Update the default dataset ID and name in train.py
          sed -i "s/parser.add_argument('--dataset-id', type=int, default=[0-9]*/parser.add_argument('--dataset-id', type=int, default=${DATASET_ID}/" train_temp.py
          sed -i "s/parser.add_argument('--task-name', type=str, default='Dataset[^']*'/parser.add_argument('--task-name', type=str, default='Dataset${DATASET_ID}_${DATASET_NAME}'/" train_temp.py
          
          # Build the Docker image with modified train.py
          cat <<EOF > Dockerfile.dataset
          FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04
          WORKDIR /opt/ml/code
          
          # Install Python and dependencies
          RUN apt-get update && apt-get install -y \
              python3 python3-pip python3-dev build-essential gcc \
              && rm -rf /var/lib/apt/lists/*
          
          RUN pip install --upgrade pip setuptools wheel build
          RUN ln -s /usr/bin/python3 /usr/bin/python
          
          # Copy requirements and install
          COPY requirements.txt ./
          RUN pip3 install --no-cache-dir -r requirements.txt
          
          # Copy the modified train.py with correct defaults
          COPY train_temp.py ./train.py
          COPY train /opt/ml/code/train
          COPY preprocessing_config.py ./
          
          # Make scripts executable
          RUN chmod +x train.py train
          RUN cp /opt/ml/code/train /usr/local/bin/train && chmod +x /usr/local/bin/train
          
          # Environment variables
          ENV SAGEMAKER_PROGRAM=train.py
          ENV SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/code
          ENV nnUNet_raw_data_base=/opt/ml/input/data/nnUNet_raw
          ENV nnUNet_preprocessed=/opt/ml/input/data/nnUNet_preprocessed
          ENV nnUNet_results=/opt/ml/model
          
          # Set dataset-specific environment variables
          ENV DEFAULT_DATASET_ID=${DATASET_ID}
          ENV DEFAULT_DATASET_NAME=Dataset${DATASET_ID}_${DATASET_NAME}
          EOF
          
          # Build with dataset-specific tag
          docker build -f Dockerfile.dataset -t $ECR_REGISTRY/$ECR_REPOSITORY:dataset-${DATASET_ID} .
          
          # Also tag with dataset name for convenience
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:dataset-${DATASET_ID} \
                     $ECR_REGISTRY/$ECR_REPOSITORY:${DATASET_NAME,,}
          
          # Push both tags
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:dataset-${DATASET_ID}
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:${DATASET_NAME,,}
          
          # If this is dataset 302, also tag as latest
          if [ "${DATASET_ID}" = "302" ]; then
            docker tag $ECR_REGISTRY/$ECR_REPOSITORY:dataset-${DATASET_ID} \
                       $ECR_REGISTRY/$ECR_REPOSITORY:latest
            docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
            echo "âœ… Tagged dataset-302 as latest"
          fi
          
          echo "âœ… Successfully pushed:"
          echo "   - $ECR_REGISTRY/$ECR_REPOSITORY:dataset-${DATASET_ID}"
          echo "   - $ECR_REGISTRY/$ECR_REPOSITORY:${DATASET_NAME,,}"
          
          # Clean up
          rm train_temp.py Dockerfile.dataset

  summary:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Build Summary
        run: |
          echo "## ðŸ“Š Build Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Dataset ID | Name | Docker Tag |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|------|------------|" >> $GITHUB_STEP_SUMMARY
          echo "| 300 | BoneLoss | \`dataset-300\`, \`boneloss\` |" >> $GITHUB_STEP_SUMMARY
          echo "| 301 | CoreAnatomicalStructures | \`dataset-301\`, \`coreanatomicalstructures\` |" >> $GITHUB_STEP_SUMMARY
          echo "| 302 | ImplantFeatures | \`dataset-302\`, \`implantfeatures\`, \`latest\` |" >> $GITHUB_STEP_SUMMARY
          echo "| 303 | ClinicalTreatmentPathology | \`dataset-303\`, \`clinicaltreatmentpathology\` |" >> $GITHUB_STEP_SUMMARY
          echo "| 304 | SpecializedClasses | \`dataset-304\`, \`specializedclasses\` |" >> $GITHUB_STEP_SUMMARY
          echo "| 305 | AdultTeeth | \`dataset-305\`, \`adultteeth\` |" >> $GITHUB_STEP_SUMMARY
          echo "| 306 | PrimaryTeeth | \`dataset-306\`, \`primaryteeth\` |" >> $GITHUB_STEP_SUMMARY
          echo "| 307 | ToothSurfacesBoundaries | \`dataset-307\`, \`toothsurfacesboundaries\` |" >> $GITHUB_STEP_SUMMARY